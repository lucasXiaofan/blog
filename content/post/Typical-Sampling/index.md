---
title: What is Typical Sampling
summary: journey from speculative sampling to typical sampling
date: 2024-3-22
authors:
  - admin
tags:
  - Transformer Inference
  - Truncate Sampling
image:
  caption: 'Image credit: [**Unsplash**](https://unsplash.com)'
---

# ğŸ¯Motivation:
1. speculative sampling[[#ğŸª™references| 2]] [[2| ]]æ˜¯ä¸€ä¸ªå¾ˆéº»çƒ¦çš„sampling æ–¹æ³•ï¼Œå…¶ä¸­æœ€è®©æˆ‘æƒ³è¦ä¼˜åŒ–çš„æ–¹é¢å°±æ˜¯speculative samplingéœ€è¦æ¯”è¾ƒtarget modelå’Œdraft modelçš„logitsã€‚ç”±äºæˆ‘ç›®å‰çš„ç ”ç©¶æ˜¯å¼‚æ„å¤§æ¨¡å‹æ¨ç†åŠ é€Ÿï¼Œäº’ç›¸ä¼ è¾“logitsæ˜¯ä¸€ä¸ªå¯ä»¥è¢«ä¼˜åŒ–çš„ç‚¹ï¼Œè€Œä»medusa [[#ğŸª™references| 1]]  è®ºæ–‡æå‡ºçš„typical samplingå°±æ²¡æœ‰äº’ç›¸ä¼ è¾“logitsçš„å¿…è¦ï¼Œæ‰€ä»¥æˆ‘æƒ³æ›´æ·±å…¥çš„å­¦ä¹ typical samplingçœ‹çœ‹å®ƒçš„æ•°å­¦è§£é‡Šï¼Œå¹¶å¸Œæœ›æœªæ¥èƒ½ç”¨LLMçš„benchmarkæ¥æµ‹è¯•typical samplingå’Œspeculative samplingçš„å·®å¼‚
2. 
# âœ… Prerequisiteï¼š
1. ç†è§£speculative sampling/decoding
3. ç†è§£transformer inference
4. åŸºç¡€çš„machine learning çŸ¥è¯†
	1. softmax
	2. logits
# ğŸ¤¨Expectationï¼š 
1. è¿™åªæ˜¯æˆ‘å­¦ä¹ typical samplingçš„å­¦ä¹ ç¬”è®°ï¼Œæˆ‘è¿˜æ²¡æœ‰å®Œå…¨ç†è§£é€å½»typical samplingæ˜¯ä»€ä¹ˆï¼Œè‹¥æœ‰é”™è¯¯å’Œä¸æ‡‚çš„ï¼Œæ¬¢è¿æŒ‡æ­£ä¸è®¨è®º
2. è¿™ç¯‡æ–‡ç« ä¼šå¾ˆé•¿ï¼Œè€Œä¸”å¿…éœ€å¾ˆé•¿ï¼Œå› ä¸ºè¿™å°±æ˜¯ç§‘ç ”çš„åšé‡ï¼ŒçŸ­ä¸€ç‚¹éƒ½ä¼šäº§ç”Ÿå¾ˆå¤šç–‘æƒ‘

---
# Content: 
## ä½¿ç”¨typical samplingçš„åŠ¨æœºï¼š
medusa [[#ğŸª™references| 1]]  è®ºæ–‡æå‡ºäº†typical acceptanceè¿™ä¸ªæ¦‚å¿µï¼Œä¸»è¦åŸå› æ˜¯: " speculative sampling results in diminished efficiency as the sampling temperature increases " ç„¶åmedusa ç»™è¿™ä¸ªåŸå› çš„æ›´æ·±çš„è§£é‡Šæ˜¯å³æ—¶draft model å’Œtarget modelä¸€æ¨¡ä¸€æ ·ï¼Œå› ä¸ºdraft å’Œtarget model â€œsample independentlyâ€ draft modelçš„ç»“æœè¿˜æ˜¯ä¼šè¢«target model æ‹’ç»æ‰ã€‚

è¿™æ˜¯æ–‡ç« ä¸­çš„åŸè¯
```
In speculative decoding papers [Leviathan et al., 2022, Chen et al., 2023], authors employ rejection sampling to yield diverse outputs that align with the distribution of the original model. However, subsequent implementations [Joao Gante, 2023, Spector and Re, 2023] reveal that this sampling strategy results in diminished efficiency as the sampling temperature increases. Intuitively, this can be comprehended in the extreme instance where the draft model is the same as the original one. Here, when using greedy decoding, all output of the draft model will be accepted, therefore maximizing the efficiency. Conversely, rejection sampling introduces extra overhead, as the draft model and the original model are sampled independently. Even if their distributions align perfectly, the output of the draft model may still be rejected.
```

å°±è¿™ä¸€æ®µè¯ï¼Œä¿¡æ¯é‡æå¤§ï¼Œéœ€è¦å½»åº•ç†è§£
1. speculative decodingçš„algorithmå’Œæ•°å­¦è¯æ˜
2. ä»¥åŠtemperature åœ¨transformer inferenceé‡Œé¢ç©¶ç«Ÿæœ‰ä»€ä¹ˆä½œç”¨ï¼Œ


å…¶å®è¯´å®è¯ï¼Œåœ¨æˆ‘å†™è¿™ä¸ªlearning noteçš„æ—¶å€™ï¼Œæˆ‘æ‡‚ speculative decodingçš„algorithmå’Œæ•°å­¦è¯æ˜ï¼Œä½†æ²¡æ‡‚é€ï¼Œè¿˜å·®ä¸´é—¨ä¸€è„šï¼Œè‡³äºtemperatureåœ¨transformer inferenceé‡Œé¢åˆ°åº•æœ‰ä»€ä¹ˆç”¨ï¼Œæˆ‘å°±åªçŸ¥é“éå¸¸æ¨¡ç³Šçš„æ¦‚å¿µï¼š
1. ä»€ä¹ˆtemperatureå’Œtransformer çš„creativityæœ‰å…³å•Š
2. temperature åœ¨softmaxé‡Œé¢æœ‰ç”¨åˆ°

#### temperature åœ¨transformer inferenceé‡Œé¢æ„å‘³ç€ä»€ä¹ˆï¼š 
æˆ‘ä»¬å…ˆæ¥è§£é‡Štemperatureï¼Œ
ç›´æ¥google â€œtemperature in transformer inferenceâ€ å°±æœ‰ä¸€ä¸ªå¾ˆå¥‘åˆæˆ‘ä»¬çš„æ¥è‡ªhuggingface forumçš„ç­”æ¡ˆ https://discuss.huggingface.co/t/what-is-temperature/11924 ï¼Œåœ¨è¿™ä¸ªç­”æ¡ˆä¸­æœ‰ä¸¤ä¸ªèµ„æºå¯ä»¥è§£é‡Šæˆ‘ä»¬çš„ç­”æ¡ˆ
1. stackoverflowçš„ç­”æ¡ˆï¼š
	1. https://stackoverflow.com/questions/58764619/why-should-we-use-temperature-in-softmax/63471046#63471046
2. huggingface çš„blogï¼Œè¿™ä¸ªç­”æ¡ˆå°±è·Ÿè´´åˆ‡transformer inferenceï¼Œè€Œä¸”è¿˜è®²äº†temperatureä¹‹å¤–çš„ï¼Œå’Œtransformer inferenceæœ‰å…³çš„å†…å®¹ï¼Œæ¯”å¦‚greedy searchï¼Œtop-kï¼Œtop-pï¼š 
	1. https://huggingface.co/blog/how-to-generate
è¯»å®Œä¸¤ä¸ªresourceä¹‹åæ€»ç»“ä¸€ä¸‹ï¼Œtemperatureçš„ä¸»è¦ä»»åŠ¡æ˜¯æ§åˆ¶ä¸€ä¸ªdistributionçš„confidence levelã€‚temperatureè¶Šé«˜ï¼Œdistributionçš„confidence levelå°±ä½ï¼Œè€Œtemperatureè¶Šä½ï¼ˆè‡³å°‘è¦å¤§äº0ï¼‰ï¼Œdistributionçš„confidence levelå°±é«˜

æ¯”å¦‚è¯´ï¼šï¼ˆä»stackoverflowçš„ç­”æ¡ˆé‡Œé¢çš„ä¾‹å­ï¼‰
[0.01,0.01,0.98] å°±æ˜¯ä¸€ä¸ªconfidence leveléå¸¸é«˜çš„distributionï¼Œsample 100 éï¼Œé¢„è®¡98æ¬¡éƒ½ä¼šæ˜¯é€‰æ‹©0.98æ¦‚ç‡çš„é€‰é¡¹

[0.2,0.2,0.6] å°±æ˜¯ä¸€ä¸ªç›¸å¯¹confidence levelæ¯•ç«Ÿä½çš„distributionï¼Œsample 100 éï¼Œé¢„è®¡60æ¬¡å›é€‰0.6çš„é€‰é¡¹

è€Œtemperatureåœ¨ç é‡Œé¢æ€ä¹ˆå®ç°å‘¢ï¼Ÿ
``` python
logits = logits / temperature
probs = F.softmax(logits, dim=1)
```
å°±softmaxä¹‹å‰æŠŠlogitsé™¤ä»¥temperatureå°±å¯ä»¥äº†
æ‰€ä»¥æ§åˆ¶confidence levelçš„æ„ä¹‰æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿæ„ä¹‰æ˜¯è®©text generationèƒ½æœ‰æ›´å¤šä¸åŒçš„ç”Ÿæˆï¼Œå¦‚æœæ¯ä¸€æ¬¡LLMçš„output distributionçš„confidence leveléƒ½éå¸¸é«˜ï¼Œé‚£ä¹ˆç”Ÿæˆçš„æ–‡å­—å°±ä¼šå‡ºç°å¾ˆå¤šé‡å¤çš„ï¼Œè§„å¾‹æ€§è¿‡å¼ºçš„ç‰¹å¾ã€‚å½“ç„¶ç”Ÿæˆæ–‡å­—çš„è´¨é‡ï¼Œæ·±ç©¶çš„è¯å°±æ˜¯ç„å­¦äº†ï¼Œå»ºè®®æ˜¯ç›´æ¥è·‘ä¸€ä¸ªbenchmarkçœ‹çœ‹åˆ†æ•°ã€‚

ä½†æ˜¯è¯è°ˆåˆ°è¿™é‡Œï¼Œæˆ‘å…¶å®æ˜¯æœ‰ä¸€ä¸ªç–‘é—®çš„ï¼Œå°±æ˜¯å½“å‰LLM pretrainçš„objective functionå…¶å®éå¸¸ç®€å•ï¼Œç›®çš„å°±æ˜¯maximize negative log likelihood of next tokenï¼Œè€Œè¿™ä¹ˆåšçš„ç›®çš„å°±æ˜¯ä¸ºäº†è®©æ¯ä¸€ä¸ªtransformer output distributionå˜å¾—éå¸¸confidentï¼Œä½†å¾ˆå¤šæ—¶å€™æˆ‘ä»¬åˆè¦æ§åˆ¶confidenceä»¥åŠå…¶ä»–çš„å¥‡æŠ€æ·«å·§æ¥è·å¾—æ›´é«˜è´¨é‡çš„æ–‡å­—ç”Ÿæˆç»“æœï¼Œè¿™æ˜¯ä¸æ˜¯æ°æ°è¯´æ˜äº†ç›®å‰çš„objective functionå¤ªè‚¤æµ…äº†ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ›´æœ‰è§£é‡Šæ€§çš„ï¼Œæ›´å¤æ‚çš„objective function æ¥åšLLMçš„pretrainï¼Ÿ

âš ï¸ ä¸è¿‡ï¼Œå¦‚æœä½ è¯»huggingface transformer inference blogå’Œmedusaçš„paperæ¯•ç«Ÿä»”ç»†çš„è¯ä¸éš¾å‘ç°ï¼Œä»–ä»¬éƒ½è¯´å½“temperature =0çš„æ—¶å€™samplingæ˜¯greedyã€‚
from huggingface transformer inference blog: 
```
OK. There are less weird n-grams and the output is a bit more coherent now! While applying temperature can make a distribution less random, in its limit, when settingÂ `temperature`Â â†’0â†’0, temperature scaled sampling becomes equal to greedy decoding and will suffer from the same problems as before.
```

from medusa paper: 
```
Examining this scheme leads to several insights. Firstly, when the temperature is set to 0, it reverts to greedy decoding, as only the most probable token possesses non-zero probability. As the temperature surpasses 0, the outcome of greedy decoding will consistently be accepted with appropriate Ïµ, Î´, since those tokens have the maximum probability, yielding maximal speedup. Likewise, in general scenarios, an increased temperature will correspondingly result in longer accepted sequences, as corroborated by our experimental findings.
```

ä½†æ˜¯åœ¨stackoverflowå¯¹temperatureçš„è§£ç­”é‡Œé¢temperature =1çš„æ—¶å€™ï¼Œsamplingæ‰æ˜¯greedy ï¼ˆè™½ç„¶å¯¹stackoverflowçš„ç­”æ¡ˆé‡Œé¢æ²¡æœ‰è¯´greedyï¼Œä½†åªè¦logitsï¼Œsoftmaxed distributionæ²¡æœ‰ä»»ä½•æ”¹åŠ¨ï¼Œç›´æ¥sample highest probability å°±æ˜¯greedyï¼‰
```
if it is 1, the output distribution will be the same as your normal softmax outputs
```
è€Œhuggingfaceè‡ªå·±çš„transformer libraryå¯¹temperatureçš„å®šä¹‰æ˜¯è¿™æ ·çš„ï¼š
https://huggingface.co/transformers/v3.4.0/_modules/transformers/generation_utils.html
``` python
if do_sample: # Temperature (higher temperature => more likely to sample low probability tokens) 
	if temperature != 1.0: 
		scores = scores / temperature
```
ä¸éš¾çœ‹å‡ºï¼Œé¦–å…ˆtemperatureè‚¯å®šä¸èƒ½æ˜¯0ï¼Œå…¶æ¬¡temperature = 1ï¼Œå¯¹distributionæ²¡æœ‰å½±å“ï¼Œä½†ç€ä¹Ÿä¸æ˜¯ä»€ä¹ˆå¤§äº‹ï¼Œ0ï¼Œå’Œ1å°±æ˜¯ä¸€ä¸ªä¸åŒäººå†™ç çš„conventionå§ã€‚è€Œä¸”è¿™æ ·ä¸å¦¨ç¢ï¼Œtemperature é«˜ï¼Œconfidence levelä½ï¼Œtemperatureä½ï¼Œconfidence level é«˜
```
exp(6) ~ 403
exp(3) ~ 20

t = 1.5
exp(6/1.5) ~ 54
exp(3/1.5) ~ 7.4

t = 0.5
exp(6/0.5) ~ 162754.791419
exp(3/0.5) ~ 403.428793493
```
